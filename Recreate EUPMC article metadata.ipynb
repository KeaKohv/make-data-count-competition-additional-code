{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436cd5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0fbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the metadata file \"PMCLiteMetadata.tgz\" from here: https://europepmc.org/pub/databases/pmc/PMCLiteMetadata/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66579aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upzip PMCLiteMetadata.tgz\n",
    "\n",
    "import tarfile\n",
    "\n",
    "# Path to the .tgz file\n",
    "tgz_path = \"PMCLiteMetadata.tgz\" # Update the path if needed\n",
    "\n",
    "# Extract the .tgz file\n",
    "with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=\"extracted_pmc_metadata\")\n",
    "    print(f\"Extracted files to: extracted_pmc_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tarfile\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ultra-fast version using lxml (much faster than xml.etree)\n",
    "try:\n",
    "    from lxml import etree\n",
    "    LXML_AVAILABLE = True\n",
    "    print(\"lxml is available - using high-performance XML parser\")\n",
    "except ImportError:\n",
    "    LXML_AVAILABLE = False\n",
    "    print(\"lxml not available - using standard parser (slower)\")\n",
    "\n",
    "def parse_pmc_xml_robust(xml_file, max_articles=None):\n",
    "    \"\"\"\n",
    "    Robust parser that can handle malformed XML with missing closing tags\n",
    "    Uses text processing to extract article data even when XML structure is broken\n",
    "    \"\"\"\n",
    "    \n",
    "    file_size = os.path.getsize(xml_file)\n",
    "    estimated_articles = file_size // 4000\n",
    "    \n",
    "    if max_articles:\n",
    "        estimated_articles = min(estimated_articles, max_articles)\n",
    "    \n",
    "    print(f\"File: {xml_file}\")\n",
    "    print(f\"Size: {file_size:,} bytes\")\n",
    "    print(f\"Estimated articles: ~{estimated_articles:,}\")\n",
    "    print(\"Starting robust extraction (handles malformed XML)...\")\n",
    "    \n",
    "    data = []\n",
    "    article_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        with open(xml_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Split content by PMC_ARTICLE tags\n",
    "        article_sections = content.split('<PMC_ARTICLE>')\n",
    "        \n",
    "        print(f\"Found {len(article_sections)-1} article sections\")\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(total=min(len(article_sections)-1, estimated_articles), desc=\"Extracting articles\", unit=\"articles\")\n",
    "        \n",
    "        for i, section in enumerate(article_sections[1:], 1):  # Skip first empty section\n",
    "            if max_articles and article_count >= max_articles:\n",
    "                break\n",
    "            \n",
    "            # Extract data using regex patterns instead of XML parsing\n",
    "            article_data = extract_article_data_regex(section)\n",
    "            if article_data:\n",
    "                data.append(article_data)\n",
    "            \n",
    "            article_count += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Update progress every 1000 articles\n",
    "            if article_count % 1000 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = article_count / elapsed if elapsed > 0 else 0\n",
    "                pbar.set_postfix({\n",
    "                    'Rate': f'{rate:.1f}/s',\n",
    "                    'Valid': len(data)\n",
    "                })\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")\n",
    "        if 'pbar' in locals():\n",
    "            pbar.close()\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nProcessing completed in {elapsed_time/60:.2f} minutes\")\n",
    "    print(f\"Processed {article_count:,} articles\")\n",
    "    print(f\"Extracted {len(data):,} valid articles\")\n",
    "    if elapsed_time > 0:\n",
    "        print(f\"Rate: {article_count/elapsed_time:.1f} articles/second\")\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def extract_article_data_regex(section):\n",
    "    \"\"\"\n",
    "    Extract article data using regex patterns instead of XML parsing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import re\n",
    "        \n",
    "        # Extract basic metadata using regex\n",
    "        article_id = extract_tag_content(section, 'id')\n",
    "        pmid = extract_tag_content(section, 'pmid')\n",
    "        pmcid = extract_tag_content(section, 'pmcid')\n",
    "        doi = extract_tag_content(section, 'DOI')\n",
    "        title = extract_tag_content(section, 'title')\n",
    "        \n",
    "        # Extract journal info\n",
    "        journal_title = extract_tag_content(section, 'JournalTitle')\n",
    "        pub_year = extract_tag_content(section, 'PubYear')\n",
    "        journal_volume = extract_tag_content(section, 'JournalVolume')\n",
    "        issue = extract_tag_content(section, 'Issue')\n",
    "        page_info = extract_tag_content(section, 'PageInfo')\n",
    "        \n",
    "        # Extract publication info\n",
    "        pub_type = extract_tag_content(section, 'PubType')\n",
    "        is_open_access = extract_tag_content(section, 'IsOpenAccess')\n",
    "        \n",
    "        # Extract authors\n",
    "        authors = extract_authors_regex(section)\n",
    "        \n",
    "        return {\n",
    "            'article_id': article_id or '',\n",
    "            'pmid': pmid or '',\n",
    "            'pmcid': pmcid or '',\n",
    "            'doi': doi or '',\n",
    "            'title': title or '',\n",
    "            'journal_title': journal_title or '',\n",
    "            'pub_year': pub_year or '',\n",
    "            'journal_volume': journal_volume or '',\n",
    "            'issue': issue or '',\n",
    "            'page_info': page_info or '',\n",
    "            'pub_type': pub_type or '',\n",
    "            'is_open_access': is_open_access or '',\n",
    "            'authors': authors or ''\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def extract_tag_content(text, tag_name):\n",
    "    \"\"\"Extract content between XML tags using regex\"\"\"\n",
    "    import re\n",
    "    pattern = f'<{tag_name}>(.*?)</{tag_name}>'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    \n",
    "    # Also try self-closing pattern\n",
    "    pattern = f'<{tag_name}>(.*?)<'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        content = match.group(1).strip()\n",
    "        # Make sure we didn't capture across tags\n",
    "        if '<' not in content:\n",
    "            return content\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_authors_regex(section):\n",
    "    \"\"\"Extract author information using regex\"\"\"\n",
    "    import re\n",
    "    \n",
    "    authors = []\n",
    "    \n",
    "    # Find AuthorList section - be more flexible with the pattern\n",
    "    author_list_patterns = [\n",
    "        r'<AuthorList[^>]*>(.*?)(?=</AuthorList>)',\n",
    "        r'<AuthorList[^>]*>(.*?)(?=<Journal)',\n",
    "        r'<AuthorList[^>]*>(.*?)(?=<[A-Z][a-z])',\n",
    "    ]\n",
    "    \n",
    "    author_list_content = None\n",
    "    for pattern in author_list_patterns:\n",
    "        author_list_match = re.search(pattern, section, re.DOTALL)\n",
    "        if author_list_match:\n",
    "            author_list_content = author_list_match.group(1)\n",
    "            break\n",
    "    \n",
    "    if not author_list_content:\n",
    "        return ''\n",
    "    \n",
    "    # Extract individual authors - use the pattern that we know works\n",
    "    author_pattern = r'<Author[^>]*>(.*?)(?=</Author>)'\n",
    "    author_matches = re.finditer(author_pattern, author_list_content, re.DOTALL)\n",
    "    \n",
    "    for author_match in author_matches:\n",
    "        author_content = author_match.group(1)\n",
    "        \n",
    "        # Extract LastName and Initials\n",
    "        last_name = extract_tag_content(author_content, 'LastName') or ''\n",
    "        initials = extract_tag_content(author_content, 'Initials') or ''\n",
    "        collective_name = extract_tag_content(author_content, 'CollectiveName') or ''\n",
    "        \n",
    "        # Clean up collective names that might have escaped characters\n",
    "        if collective_name:\n",
    "            collective_name = collective_name.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')\n",
    "            authors.append(collective_name)\n",
    "        elif last_name or initials:\n",
    "            author_name = f\"{last_name}, {initials}\".strip(', ')\n",
    "            if author_name and author_name not in authors:  # Avoid duplicates\n",
    "                authors.append(author_name)\n",
    "    \n",
    "    return '; '.join(authors)\n",
    "\n",
    "class CleaningFileWrapper:\n",
    "    \"\"\"File wrapper that cleans invalid XML characters on-the-fly\"\"\"\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.file = open(file_path, 'rb')  # Open in binary mode\n",
    "        \n",
    "        # Precompiled regex for invalid XML characters (bytes pattern)\n",
    "        # XML 1.0 valid: tab(9), LF(10), CR(13), and chars 32-126, plus UTF-8 sequences\n",
    "        self.invalid_chars_regex = re.compile(b'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x84\\x86-\\x9F]')\n",
    "        \n",
    "        # Regex to fix unescaped ampersands (& not followed by valid entity references)\n",
    "        # Valid XML entities: &amp; &lt; &gt; &quot; &apos; &#number; &#xhex;\n",
    "        self.ampersand_regex = re.compile(b'&(?!(?:amp|lt|gt|quot|apos|#(?:\\d+|x[0-9a-fA-F]+));)')\n",
    "        \n",
    "        # Regex to fix unescaped < and > characters in text content\n",
    "        # This is more complex - we need to avoid breaking real XML tags\n",
    "        # We'll look for < that's not followed by valid tag patterns\n",
    "        self.unescaped_lt_regex = re.compile(b'<(?![/?]?[a-zA-Z_][a-zA-Z0-9_:-]*[^>]*>)')\n",
    "        self.unescaped_gt_regex = re.compile(b'(?<![a-zA-Z0-9_:-])>(?![^<]*</)')\n",
    "        \n",
    "        self.chars_cleaned = 0\n",
    "        self.ampersands_fixed = 0\n",
    "        self.lt_fixed = 0\n",
    "        self.gt_fixed = 0\n",
    "    \n",
    "    def read(self, size=-1):\n",
    "        chunk = self.file.read(size)\n",
    "        if chunk:\n",
    "            # Remove invalid characters on-the-fly\n",
    "            original_len = len(chunk)\n",
    "            cleaned_chunk = self.invalid_chars_regex.sub(b'', chunk)\n",
    "            self.chars_cleaned += original_len - len(cleaned_chunk)\n",
    "            \n",
    "            # Fix unescaped ampersands\n",
    "            original_amp_len = len(cleaned_chunk)\n",
    "            cleaned_chunk = self.ampersand_regex.sub(b'&amp;', cleaned_chunk)\n",
    "            amp_matches = len(re.findall(b'&amp;', cleaned_chunk)) - len(re.findall(b'&amp;', chunk))\n",
    "            self.ampersands_fixed += amp_matches\n",
    "            \n",
    "            # Fix unescaped < characters (broader pattern)\n",
    "            # Look for < that's not part of valid XML tags\n",
    "            # This catches cases like \"B<FIT\" or \"<2.66\" in text content\n",
    "            lt_pattern = re.compile(b'<(?![/?]?[a-zA-Z_][a-zA-Z0-9_:-]*(?:\\s[^>]*)?>)')\n",
    "            lt_matches = len(re.findall(lt_pattern, cleaned_chunk))\n",
    "            cleaned_chunk = lt_pattern.sub(b'&lt;', cleaned_chunk)\n",
    "            self.lt_fixed += lt_matches\n",
    "            \n",
    "            # Fix missing </PMC_ARTICLE> closing tags\n",
    "            # Look for patterns like </PMC_ARTICLE><PMC_ARTICLE> that should be there\n",
    "            # but are missing the closing tag before starting a new article\n",
    "            article_pattern = re.compile(b'<PMC_ARTICLE><id>')\n",
    "            if b'<PMC_ARTICLE><id>' in cleaned_chunk and not cleaned_chunk.startswith(b'<PMC_ARTICLE><id>'):\n",
    "                # Insert missing closing tags where needed\n",
    "                cleaned_chunk = article_pattern.sub(b'</PMC_ARTICLE><PMC_ARTICLE><id>', cleaned_chunk)\n",
    "            \n",
    "            return cleaned_chunk\n",
    "        return chunk\n",
    "    \n",
    "    def close(self):\n",
    "        self.file.close()\n",
    "        if self.chars_cleaned > 0:\n",
    "            print(f\"Cleaned {self.chars_cleaned:,} invalid XML characters on-the-fly\")\n",
    "        if self.ampersands_fixed > 0:\n",
    "            print(f\"Fixed {self.ampersands_fixed:,} unescaped ampersands on-the-fly\")\n",
    "        if self.lt_fixed > 0:\n",
    "            print(f\"Fixed {self.lt_fixed:,} unescaped < characters on-the-fly\")\n",
    "    \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()\n",
    "\n",
    "def extract_article_data_lxml(elem):\n",
    "    \"\"\"\n",
    "    Extract article data from a PMC_ARTICLE element using lxml methods\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract basic metadata\n",
    "        article_id = elem.findtext('.//id', default='')\n",
    "        pmid = elem.findtext('.//pmid', default='')\n",
    "        pmcid = elem.findtext('.//pmcid', default='')\n",
    "        doi = elem.findtext('.//DOI', default='')\n",
    "        title = elem.findtext('.//title', default='')\n",
    "        \n",
    "        # Extract journal info\n",
    "        journal_title = elem.findtext('.//JournalTitle', default='')\n",
    "        pub_year = elem.findtext('.//PubYear', default='')\n",
    "        journal_volume = elem.findtext('.//JournalVolume', default='')\n",
    "        issue = elem.findtext('.//Issue', default='')\n",
    "        page_info = elem.findtext('.//PageInfo', default='')\n",
    "        \n",
    "        # Extract publication info\n",
    "        pub_type = elem.findtext('.//PubType', default='')\n",
    "        is_open_access = elem.findtext('.//IsOpenAccess', default='')\n",
    "        \n",
    "        # Extract author information\n",
    "        authors = []\n",
    "        author_list = elem.find('.//AuthorList')\n",
    "        if author_list is not None:\n",
    "            for author in author_list.findall('.//Author'):\n",
    "                last_name = author.findtext('.//LastName', default='')\n",
    "                initials = author.findtext('.//Initials', default='')\n",
    "                collective_name = author.findtext('.//CollectiveName', default='')\n",
    "                \n",
    "                if collective_name:\n",
    "                    # Clean up escaped characters in collective names\n",
    "                    collective_name = collective_name.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')\n",
    "                    authors.append(collective_name)\n",
    "                elif last_name or initials:\n",
    "                    author_name = f\"{last_name}, {initials}\".strip(', ')\n",
    "                    if author_name:  # Only add non-empty names\n",
    "                        authors.append(author_name)\n",
    "        \n",
    "        authors_str = '; '.join(authors)\n",
    "        \n",
    "        return {\n",
    "            'article_id': article_id,\n",
    "            'pmid': pmid,\n",
    "            'pmcid': pmcid,\n",
    "            'doi': doi,\n",
    "            'title': title,\n",
    "            'journal_title': journal_title,\n",
    "            'pub_year': pub_year,\n",
    "            'journal_volume': journal_volume,\n",
    "            'issue': issue,\n",
    "            'page_info': page_info,\n",
    "            'pub_type': pub_type,\n",
    "            'is_open_access': is_open_access,\n",
    "            'authors': authors_str\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Return None if extraction fails\n",
    "        return None\n",
    "\n",
    "# Ultra-fast parser with on-the-fly character cleaning\n",
    "def parse_pmc_xml_lxml_safe(xml_file, max_articles=None):\n",
    "    \"\"\"\n",
    "    Ultra-fast parser for PMC XML files using lxml with on-the-fly character cleaning\n",
    "    \"\"\"\n",
    "    \n",
    "    if not LXML_AVAILABLE:\n",
    "        print(\"lxml not available, falling back to standard parser\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    file_size = os.path.getsize(xml_file)\n",
    "    estimated_articles = file_size // 4000\n",
    "    \n",
    "    if max_articles:\n",
    "        estimated_articles = min(estimated_articles, max_articles)\n",
    "    \n",
    "    print(f\"File: {xml_file}\")\n",
    "    print(f\"Size: {file_size:,} bytes\")\n",
    "    print(f\"Estimated articles: ~{estimated_articles:,}\")\n",
    "    print(\"Starting extraction with lxml and on-the-fly cleaning...\")\n",
    "    \n",
    "    data = []\n",
    "    article_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Use cleaning file wrapper for on-the-fly character cleaning\n",
    "        with CleaningFileWrapper(xml_file) as clean_file:\n",
    "            # Use lxml iterparse with the cleaning wrapper\n",
    "            context = etree.iterparse(clean_file, events=('start', 'end'), tag='PMC_ARTICLE')\n",
    "            \n",
    "            # Progress bar\n",
    "            pbar = tqdm(total=estimated_articles, desc=\"Extracting articles\", unit=\"articles\")\n",
    "            \n",
    "            for event, elem in context:\n",
    "                if event == 'end':\n",
    "                    if max_articles and article_count >= max_articles:\n",
    "                        break\n",
    "                    \n",
    "                    # Extract article data using lxml methods\n",
    "                    article_data = extract_article_data_lxml(elem)\n",
    "                    if article_data:\n",
    "                        data.append(article_data)\n",
    "                    \n",
    "                    article_count += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                    # Update progress every 1000 articles\n",
    "                    if article_count % 1000 == 0:\n",
    "                        elapsed = time.time() - start_time\n",
    "                        rate = article_count / elapsed if elapsed > 0 else 0\n",
    "                        pbar.set_postfix({\n",
    "                            'Rate': f'{rate:.1f}/s',\n",
    "                            'Valid': len(data)\n",
    "                        })\n",
    "                    \n",
    "                    # Clear processed element to save memory\n",
    "                    elem.clear()\n",
    "                    # Also clear preceding siblings\n",
    "                    while elem.getprevious() is not None:\n",
    "                        del elem.getparent()[0]\n",
    "            \n",
    "            pbar.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML: {e}\")\n",
    "        if 'pbar' in locals():\n",
    "            pbar.close()\n",
    "        \n",
    "        # Print the problematic XML content around the error\n",
    "        try:\n",
    "            error_str = str(e)\n",
    "            # Extract line number from error message\n",
    "            line_match = re.search(r'line (\\d+)', error_str)\n",
    "            if line_match:\n",
    "                error_line = int(line_match.group(1))\n",
    "                print(f\"\\nExtracting XML content around line {error_line}...\")\n",
    "                \n",
    "                with open(xml_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    lines = []\n",
    "                    current_line = 0\n",
    "                    \n",
    "                    # Read lines around the error\n",
    "                    for line in f:\n",
    "                        current_line += 1\n",
    "                        \n",
    "                        # Keep lines around the error (±10 lines)\n",
    "                        if error_line - 10 <= current_line <= error_line + 10:\n",
    "                            lines.append(f\"Line {current_line}: {repr(line)}\")\n",
    "                        \n",
    "                        # Stop reading after we've passed the error area\n",
    "                        if current_line > error_line + 10:\n",
    "                            break\n",
    "                    \n",
    "                    print(\"\\nProblematic XML content:\")\n",
    "                    print(\"=\" * 80)\n",
    "                    for line in lines:\n",
    "                        print(line)\n",
    "                    print(\"=\" * 80)\n",
    "                    \n",
    "                    # Also show the specific character at the column position\n",
    "                    if len(lines) > 10:  # Find the error line\n",
    "                        error_line_content = lines[10]  # Middle line should be the error line\n",
    "                        column_match = re.search(r'column (\\d+)', error_str)\n",
    "                        if column_match:\n",
    "                            column = int(column_match.group(1))\n",
    "                            print(f\"\\nSpecific problematic area around column {column}:\")\n",
    "                            line_text = error_line_content.split(': ', 1)[1] if ': ' in error_line_content else error_line_content\n",
    "                            line_text = eval(line_text)  # Convert from repr back to string\n",
    "                            start_pos = max(0, column - 50)\n",
    "                            end_pos = min(len(line_text), column + 50)\n",
    "                            problematic_section = line_text[start_pos:end_pos]\n",
    "                            print(f\"Context: {repr(problematic_section)}\")\n",
    "                            if column < len(line_text):\n",
    "                                print(f\"Character at column {column}: {repr(line_text[column])}\")\n",
    "                    \n",
    "        except Exception as debug_e:\n",
    "            print(f\"Could not extract problematic XML content: {debug_e}\")\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nProcessing completed in {elapsed_time/60:.2f} minutes\")\n",
    "    print(f\"Processed {article_count:,} articles\")\n",
    "    print(f\"Extracted {len(data):,} valid articles\")\n",
    "    if elapsed_time > 0:\n",
    "        print(f\"Rate: {article_count/elapsed_time:.1f} articles/second\")\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def process_xml_directory(xml_dir, output_dir, max_articles_per_file=None):\n",
    "    \"\"\"\n",
    "    Process all XML files in a directory and save each as a parquet file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all XML files in the directory\n",
    "    xml_files = glob.glob(os.path.join(xml_dir, \"*.xml\"))\n",
    "    \n",
    "    if not xml_files:\n",
    "        print(f\"No XML files found in directory: {xml_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(xml_files)} XML files in {xml_dir}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    successful_files = 0\n",
    "    failed_files = 0\n",
    "    total_articles = 0\n",
    "    \n",
    "    for i, xml_file_path in enumerate(xml_files, 1):\n",
    "        filename = os.path.basename(xml_file_path)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        output_path = os.path.join(output_dir, f\"{base_name}.parquet\")\n",
    "        \n",
    "        print(f\"\\n[{i}/{len(xml_files)}] Processing: {filename}\")\n",
    "        \n",
    "        # Skip if output file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"Output file already exists, skipping: {output_path}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Try the XML parser first, fall back to robust parser if it fails\n",
    "            print(\"Attempting XML-based parsing...\")\n",
    "            df_pmc = parse_pmc_xml_lxml_safe(xml_file_path, max_articles=max_articles_per_file)\n",
    "            \n",
    "            # If XML parsing failed, try the robust parser\n",
    "            if df_pmc.empty:\n",
    "                print(\"XML parsing failed due to malformed XML. Trying robust text-based parser...\")\n",
    "                df_pmc = parse_pmc_xml_robust(xml_file_path, max_articles=max_articles_per_file)\n",
    "            \n",
    "            if not df_pmc.empty:\n",
    "                # Save as parquet file\n",
    "                df_pmc.to_parquet(output_path, compression='snappy')\n",
    "                print(f\"✓ Saved {len(df_pmc):,} articles to: {output_path}\")\n",
    "                print(f\"  File size: {os.path.getsize(output_path):,} bytes\")\n",
    "                \n",
    "                successful_files += 1\n",
    "                total_articles += len(df_pmc)\n",
    "                \n",
    "                # Show some sample data for the first file\n",
    "                if i == 1:\n",
    "                    print(f\"\\nSample data from first file:\")\n",
    "                    print(f\"Columns: {list(df_pmc.columns)}\")\n",
    "                    print(f\"Shape: {df_pmc.shape}\")\n",
    "                    sample_df = df_pmc[df_pmc['doi'] != ''].head(2)\n",
    "                    if not sample_df.empty:\n",
    "                        print(\"Sample entries with DOI:\")\n",
    "                        for col in ['pmid', 'pmcid', 'doi', 'title', 'journal_title']:\n",
    "                            if col in sample_df.columns:\n",
    "                                print(f\"  {col}: {sample_df[col].iloc[0]}\")\n",
    "            else:\n",
    "                print(f\"✗ No articles extracted from: {filename}\")\n",
    "                failed_files += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing {filename}: {e}\")\n",
    "            failed_files += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total XML files found: {len(xml_files)}\")\n",
    "    print(f\"Successfully processed: {successful_files}\")\n",
    "    print(f\"Failed to process: {failed_files}\")\n",
    "    print(f\"Total articles extracted: {total_articles:,}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    if successful_files > 0:\n",
    "        # List all created parquet files\n",
    "        parquet_files = glob.glob(os.path.join(output_dir, \"*.parquet\"))\n",
    "        print(f\"\\nCreated parquet files ({len(parquet_files)}):\")\n",
    "        for pf in sorted(parquet_files):\n",
    "            size = os.path.getsize(pf)\n",
    "            print(f\"  {os.path.basename(pf)} ({size:,} bytes)\")\n",
    "\n",
    "# Set directories\n",
    "xml_directory = \"extracted_pmc_metadata/out\"\n",
    "output_directory = r\"pmc_parquet_files\"\n",
    "\n",
    "# Process all XML files in the directory\n",
    "process_xml_directory(xml_directory, output_directory, max_articles_per_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f308f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get all parquet files in the folder\n",
    "parquet_files = glob.glob(\"pmc_parquet_files/*.parquet\")\n",
    "print(f\"Found {len(parquet_files)} parquet files\")\n",
    "\n",
    "# List to store processed dataframes\n",
    "processed_dfs = []\n",
    "\n",
    "# Process each parquet file\n",
    "for file_path in parquet_files:\n",
    "    print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "    \n",
    "    # Read the parquet file\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Drop rows where doi is NaN\n",
    "    df = df.dropna(subset=['doi'])\n",
    "    \n",
    "    # Drop rows where doi is empty string\n",
    "    df = df[df['doi'].str.strip() != '']\n",
    "    \n",
    "    print(f\"  - Rows after cleaning: {len(df)}\")\n",
    "    processed_dfs.append(df)\n",
    "\n",
    "# Concatenate all processed dataframes\n",
    "print(\"Concatenating all dataframes...\")\n",
    "df_combined = pd.concat(processed_dfs, ignore_index=True)\n",
    "print(f\"Total combined rows: {len(df_combined)}\")\n",
    "\n",
    "# Optimize data types and compress for smaller file size\n",
    "metadata_df = df_combined[['pmcid','article_id','authors','title','journal_title','pub_year']].copy()\n",
    "\n",
    "# Convert pub_year to int16 if it's numeric (saves space)\n",
    "if metadata_df['pub_year'].dtype in ['float64', 'int64']:\n",
    "    metadata_df['pub_year'] = metadata_df['pub_year'].astype('int16')\n",
    "\n",
    "# Use gzip compression for size reduction\n",
    "metadata_df.to_parquet(\"PMC_article_metadata.parquet\", \n",
    "                      compression='gzip', \n",
    "                      index=False)\n",
    "\n",
    "print(f\"Saved {len(metadata_df)} rows to compressed parquet file\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
